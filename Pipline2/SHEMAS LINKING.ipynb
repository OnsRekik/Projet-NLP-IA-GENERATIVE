{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b03678-b8ad-4afd-9a19-6da04f0f6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------------------------------------PIPLINE3/SHEMAS LINKING + BART-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "722601b7-a790-4406-a83f-746acb94ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import spacy                  # pour NLP\n",
    "from rapidfuzz import fuzz    # pour fuzzy matching (plus moderne que fuzzywuzzy)\n",
    "import networkx as nx         # pour le graphe question + sch√©ma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172be35-b35e-425e-9fc3-fbc6278a4922",
   "metadata": {},
   "source": [
    "****************************I/PARTIE NLP****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542dbf5a-b460-49ba-a131-20b5c5d5201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sqlite3\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b0c545-ff6a-472c-a6bf-fce4b3bf21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"datafinal\"\n",
    "\n",
    "# === Charger train_spider.json ===\n",
    "with open(f\"{BASE_DIR}/train_spider.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e877592a-4bd5-4839-b7ec-d81177d6d997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de sch√©mas de DB : 166\n",
      "Premier sch√©ma :\n",
      "{'column_names': [[-1, '*'],\n",
      "                  [0, 'perpetrator id'],\n",
      "                  [0, 'people id'],\n",
      "                  [0, 'date'],\n",
      "                  [0, 'year'],\n",
      "                  [0, 'location'],\n",
      "                  [0, 'country'],\n",
      "                  [0, 'killed'],\n",
      "                  [0, 'injured'],\n",
      "                  [1, 'people id'],\n",
      "                  [1, 'name'],\n",
      "                  [1, 'height'],\n",
      "                  [1, 'weight'],\n",
      "                  [1, 'home town']],\n",
      " 'column_names_original': [[-1, '*'],\n",
      "                           [0, 'Perpetrator_ID'],\n",
      "                           [0, 'People_ID'],\n",
      "                           [0, 'Date'],\n",
      "                           [0, 'Year'],\n",
      "                           [0, 'Location'],\n",
      "                           [0, 'Country'],\n",
      "                           [0, 'Killed'],\n",
      "                           [0, 'Injured'],\n",
      "                           [1, 'People_ID'],\n",
      "                           [1, 'Name'],\n",
      "                           [1, 'Height'],\n",
      "                           [1, 'Weight'],\n",
      "                           [1, 'Home Town']],\n",
      " 'column_types': ['text',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'text',\n",
      "                  'number',\n",
      "                  'text',\n",
      "                  'text',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'text',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'text'],\n",
      " 'db_id': 'perpetrator',\n",
      " 'foreign_keys': [[2, 9]],\n",
      " 'primary_keys': [1, 9],\n",
      " 'table_names': ['perpetrator', 'people'],\n",
      " 'table_names_original': ['perpetrator', 'people']}\n"
     ]
    }
   ],
   "source": [
    "# === Charger tables.json ===\n",
    "with open(f\"{BASE_DIR}/tables.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "print(\"Nombre de sch√©mas de DB :\", len(tables))\n",
    "print(\"Premier sch√©ma :\")\n",
    "pprint(tables[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb9ee0-4b3c-4a42-826e-33b4b0d60a33",
   "metadata": {},
   "source": [
    "### √âtape 1 ‚Äì R√©cup√©rer le sch√©ma d‚Äôune DB √† partir de db_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efe095-e249-4af4-b824-3b670c693a19",
   "metadata": {},
   "source": [
    "On va cr√©er une fonction utilitaire pour trouver le bon sch√©ma dans tables.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4ec25f-f5ba-49fb-84c7-ddfd3c3aa11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_schema(db_id, tables_json):\n",
    "    \"\"\"\n",
    "    Retourne le sch√©ma correspondant √† un db_id donn√©,\n",
    "    ou None si non trouv√©.\n",
    "    \"\"\"\n",
    "    for db in tables_json:\n",
    "        if db[\"db_id\"] == db_id:\n",
    "            return db\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be6119-4a25-459b-9e61-0fb7f1f60dbd",
   "metadata": {},
   "source": [
    "Testons sur le premier exemple de train_spider :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a68fcbe-9cdd-45e6-9970-036422eadf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_id de l'exemple : department_management\n",
      "\n",
      "Sch√©ma associ√© :\n",
      "{'column_names': [[-1, '*'],\n",
      "                  [0, 'department id'],\n",
      "                  [0, 'name'],\n",
      "                  [0, 'creation'],\n",
      "                  [0, 'ranking'],\n",
      "                  [0, 'budget in billions'],\n",
      "                  [0, 'num employees'],\n",
      "                  [1, 'head id'],\n",
      "                  [1, 'name'],\n",
      "                  [1, 'born state'],\n",
      "                  [1, 'age'],\n",
      "                  [2, 'department id'],\n",
      "                  [2, 'head id'],\n",
      "                  [2, 'temporary acting']],\n",
      " 'column_names_original': [[-1, '*'],\n",
      "                           [0, 'Department_ID'],\n",
      "                           [0, 'Name'],\n",
      "                           [0, 'Creation'],\n",
      "                           [0, 'Ranking'],\n",
      "                           [0, 'Budget_in_Billions'],\n",
      "                           [0, 'Num_Employees'],\n",
      "                           [1, 'head_ID'],\n",
      "                           [1, 'name'],\n",
      "                           [1, 'born_state'],\n",
      "                           [1, 'age'],\n",
      "                           [2, 'department_ID'],\n",
      "                           [2, 'head_ID'],\n",
      "                           [2, 'temporary_acting']],\n",
      " 'column_types': ['text',\n",
      "                  'number',\n",
      "                  'text',\n",
      "                  'text',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'text',\n",
      "                  'text',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'number',\n",
      "                  'text'],\n",
      " 'db_id': 'department_management',\n",
      " 'foreign_keys': [[12, 7], [11, 1]],\n",
      " 'primary_keys': [1, 7, 11],\n",
      " 'table_names': ['department', 'head', 'management'],\n",
      " 'table_names_original': ['department', 'head', 'management']}\n"
     ]
    }
   ],
   "source": [
    "example = train_data[0]\n",
    "db_id = example[\"db_id\"]\n",
    "schema = get_db_schema(db_id, tables)\n",
    "\n",
    "print(\"db_id de l'exemple :\", db_id)\n",
    "print(\"\\nSch√©ma associ√© :\")\n",
    "pprint(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bc8df-b507-499a-a5ec-1bd568e37822",
   "metadata": {},
   "source": [
    "### √âtape 2 ‚Äì Afficher joli le sch√©ma (pour bien le comprendre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70000f79-166c-4c3b-9246-93186051a5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå DB : department_management\n",
      "\n",
      "Tables :\n",
      "  [0] department\n",
      "  [1] head\n",
      "  [2] management\n",
      "\n",
      "Colonnes :\n",
      "  - department.department id\n",
      "  - department.name\n",
      "  - department.creation\n",
      "  - department.ranking\n",
      "  - department.budget in billions\n",
      "  - department.num employees\n",
      "  - head.head id\n",
      "  - head.name\n",
      "  - head.born state\n",
      "  - head.age\n",
      "  - management.department id\n",
      "  - management.head id\n",
      "  - management.temporary acting\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_schema(schema):\n",
    "    \"\"\"\n",
    "    Affiche les tables et colonnes d'un sch√©ma Spider\n",
    "    de mani√®re lisible.\n",
    "    \"\"\"\n",
    "    table_names = schema[\"table_names\"]\n",
    "    col_names = schema[\"column_names\"]  # liste [ (table_idx, col_name), ... ]\n",
    "\n",
    "    print(f\"üìå DB : {schema['db_id']}\\n\")\n",
    "\n",
    "    print(\"Tables :\")\n",
    "    for i, t in enumerate(table_names):\n",
    "        print(f\"  [{i}] {t}\")\n",
    "\n",
    "    print(\"\\nColonnes :\")\n",
    "    for (table_idx, col_name) in col_names:\n",
    "        if table_idx == -1:  # l'entr√©e sp√©ciale '*'\n",
    "            continue\n",
    "        table_name = table_names[table_idx]\n",
    "        print(f\"  - {table_name}.{col_name}\")\n",
    "\n",
    "# Test sur le sch√©ma de l'exemple 0\n",
    "pretty_print_schema(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429d3a0-daee-4015-8d95-fbfff7c14d60",
   "metadata": {},
   "source": [
    "### √âtape 3 ‚Äì Pr√©processing de la question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b356f55-6112-41e9-80d7-41aecb543e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le spaCy (anglais ici car Spider est en anglais)\n",
    "# ‚ö†Ô∏è √Ä faire UNE SEULE FOIS dans ton script\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61c34237-221c-401e-a891-613363433d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenInfo:\n",
    "    \"\"\"\n",
    "    Repr√©sente un token de la question avec plusieurs infos utiles\n",
    "    pour le schema linking.\n",
    "    \"\"\"\n",
    "    text: str        # forme originale\n",
    "    lemma: str       # lemme (forme de base)\n",
    "    pos: str         # cat√©gorie grammaticale (NOUN, VERB, NUM, etc.)\n",
    "    dep: str         # relation de d√©pendance\n",
    "    head: str        # t√™te de d√©pendance (texte du token racine)\n",
    "    idx: int         # position du token dans la phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7dfc39c-36e1-479f-9e4e-defdad593599",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QuestionPreproc:\n",
    "    \"\"\"\n",
    "    Structure de donn√©es contenant toutes les infos\n",
    "    pr√©trait√©es pour une question.\n",
    "    \"\"\"\n",
    "    raw: str                     # question originale\n",
    "    normalized: str              # question normalis√©e (minuscule, etc.)\n",
    "    tokens: List[TokenInfo]      # liste des tokens annot√©s\n",
    "    values: List[str]            # valeurs candidates (nombres et strings)\n",
    "    doc: Any                     # objet spaCy Doc (si besoin plus tard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0da76836-5852-43c7-97fa-fad116136a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_question(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalise la question :\n",
    "    - minuscule\n",
    "    - nettoyage des espaces\n",
    "    - suppression de toute la ponctuation\n",
    "    \"\"\"\n",
    "    # minuscule\n",
    "    text = text.lower()\n",
    "\n",
    "    # supprimer toute la ponctuation\n",
    "    # conserve lettres, chiffres et espaces\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # r√©duire les espaces multiples\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e82feeb-30ea-46ed-b9a6-1532df3d6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values_from_doc(doc: spacy.tokens.Doc) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrait les valeurs candidates :\n",
    "    - nombres (POS = NUM)\n",
    "    - strings entre guillemets (ex : \"texas\")\n",
    "    \"\"\"\n",
    "    values = []\n",
    "\n",
    "    # 1) Nombres explicites dans le texte\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NUM\":\n",
    "            values.append(token.text)\n",
    "\n",
    "    # 2) Strings entre guillemets (simples ou doubles)\n",
    "    text = doc.text\n",
    "    string_values = re.findall(r'\"([^\"]+)\"', text) + re.findall(r\"'([^']+)'\", text)\n",
    "    values.extend(string_values)\n",
    "\n",
    "    # enlever doublons en gardant l'ordre\n",
    "    seen = set()\n",
    "    unique_values = []\n",
    "    for v in values:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            unique_values.append(v)\n",
    "\n",
    "    return unique_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d699e336-bf79-44ef-8743-f4cf673592c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_question(question: str, nlp_model) -> QuestionPreproc:\n",
    "    \"\"\"\n",
    "    Pipeline de pr√©processing complet pour UNE question :\n",
    "    - normalisation\n",
    "    - analyse spaCy\n",
    "    - extraction tokens + lemmes + POS + d√©pendances\n",
    "    - extraction des valeurs candidates\n",
    "    \"\"\"\n",
    "    raw = question\n",
    "    normalized = normalize_question(raw)\n",
    "\n",
    "    # Passage dans spaCy pour avoir POS, lemmes, d√©pendances, etc.\n",
    "    doc = nlp_model(normalized)\n",
    "\n",
    "    tokens_info: List[TokenInfo] = []\n",
    "    for i, tok in enumerate(doc):\n",
    "        tokens_info.append(\n",
    "            TokenInfo(\n",
    "                text=tok.text,\n",
    "                lemma=tok.lemma_,\n",
    "                pos=tok.pos_,\n",
    "                dep=tok.dep_,\n",
    "                head=tok.head.text,\n",
    "                idx=i\n",
    "            )\n",
    "        )\n",
    "\n",
    "    values = extract_values_from_doc(doc)\n",
    "\n",
    "    return QuestionPreproc(\n",
    "        raw=raw,\n",
    "        normalized=normalized,\n",
    "        tokens=tokens_info,\n",
    "        values=values,\n",
    "        doc=doc\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8a997ed-bd66-4735-b01c-99b76e55e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question originale : How many heads of the departments are older than 56 ?\n",
      "\n",
      "Question normalis√©e : how many heads of the departments are older than 56\n",
      "Tokens :\n",
      "  idx= 0 text=how        lemma=how        pos=SCONJ dep=advmod     head=many\n",
      "  idx= 1 text=many       lemma=many       pos=ADJ   dep=amod       head=heads\n",
      "  idx= 2 text=heads      lemma=head       pos=NOUN  dep=nsubj      head=are\n",
      "  idx= 3 text=of         lemma=of         pos=ADP   dep=prep       head=heads\n",
      "  idx= 4 text=the        lemma=the        pos=DET   dep=det        head=departments\n",
      "  idx= 5 text=departments lemma=department pos=NOUN  dep=pobj       head=of\n",
      "  idx= 6 text=are        lemma=be         pos=AUX   dep=ROOT       head=are\n",
      "  idx= 7 text=older      lemma=old        pos=ADJ   dep=acomp      head=are\n",
      "  idx= 8 text=than       lemma=than       pos=ADP   dep=prep       head=older\n",
      "  idx= 9 text=56         lemma=56         pos=NUM   dep=pobj       head=than\n",
      "\n",
      "Valeurs candidates : ['56']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Exemple d'utilisation de l'√©tape 3 sur le premier exemple\n",
    "# ============================================================\n",
    "example = train_data[0]\n",
    "question = example[\"question\"]\n",
    "print(\"Question originale :\", question)\n",
    "\n",
    "q_preproc = preprocess_question(question, nlp)\n",
    "\n",
    "print(\"\\nQuestion normalis√©e :\", q_preproc.normalized)\n",
    "print(\"Tokens :\")\n",
    "for t in q_preproc.tokens:\n",
    "    print(f\"  idx={t.idx:2d} text={t.text:10s} lemma={t.lemma:10s} pos={t.pos:5s} dep={t.dep:10s} head={t.head}\")\n",
    "\n",
    "print(\"\\nValeurs candidates :\", q_preproc.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d571b77-48f1-438c-9f3d-ec28d51861ec",
   "metadata": {},
   "source": [
    "SpaCy attribue √† chaque token un type grammatical comme :\n",
    "\n",
    "NOUN ‚Üí nom\n",
    "\n",
    "VERB ‚Üí verbe\n",
    "\n",
    "ADJ ‚Üí adjectif\n",
    "\n",
    "ADV ‚Üí adverbe\n",
    "\n",
    "NUM ‚Üí nombre\n",
    "\n",
    "DET ‚Üí d√©terminant (\"the\", \"a\")\n",
    "\n",
    "ADP ‚Üí adposition (pr√©positions comme \"of\", \"than\")\n",
    "\n",
    "AUX ‚Üí auxiliaire (\"are\", \"is\")\n",
    "\n",
    "SCONJ ‚Üí conjonction (souvent pour mots interrogatifs comme \"how\")\n",
    "\n",
    "üëâ POS sert √† identifier la fonction grammaticale basique d'un mot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c146c1-558a-4530-a5d2-d46553bc8f5b",
   "metadata": {},
   "source": [
    "DEP (Dependency Relation)\n",
    "\n",
    "‚û°Ô∏è C‚Äôest la relation syntaxique entre ce token et son mot parent (HEAD).\n",
    "Elle indique comment le mot s‚Äôint√®gre dans la phrase, par exemple :\n",
    "\n",
    "nsubj ‚Üí nom sujet (ex. \"heads\" est sujet de \"are\")\n",
    "\n",
    "pobj ‚Üí objet d‚Äôune pr√©position (ex. \"departments\" est objet de \"of\")\n",
    "\n",
    "advmod ‚Üí modificateur adverbial (ex. ‚Äúhow‚Äù modifie ‚Äúmany‚Äù)\n",
    "\n",
    "amod ‚Üí adjectif modifiant un nom (ex. ‚Äúmany‚Äù modifie ‚Äúheads‚Äù)\n",
    "\n",
    "prep ‚Üí pr√©position (\"of\", \"than\")\n",
    "\n",
    "acomp ‚Üí adjectival complement (ex. ‚Äúolder‚Äù compl√®te ‚Äúare‚Äù)\n",
    "\n",
    "ROOT ‚Üí racine de la phrase (ex. ‚Äúare‚Äù)\n",
    "\n",
    "det ‚Üí d√©terminant (ex. ‚Äúthe‚Äù ‚Üí ‚Äúdepartments‚Äù)\n",
    "\n",
    "üëâ DEP sert √† comprendre la structure grammaticale compl√®te de la phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be63dfc-5db7-4623-b80e-74bcdb7ee33a",
   "metadata": {},
   "source": [
    "Dans l‚Äô√©tape 3, nous avons d‚Äôabord normalis√© la question (minuscule, suppression de la ponctuation, nettoyage) pour obtenir un texte propre et coh√©rent. Ensuite, nous avons appliqu√© une analyse linguistique compl√®te avec spaCy (lemmes, cat√©gories grammaticales, d√©pendances et valeurs num√©riques) afin de pr√©parer toutes les informations n√©cessaires pour le schema linking des √©tapes suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ede3f5-d756-466f-85b2-107bad61bf8c",
   "metadata": {},
   "source": [
    "### √âtape 4 ‚Äì Baseline de Schema Linking (matching lexical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53e592-8a34-4ac1-8bb3-5f89d0b4cd8d",
   "metadata": {},
   "source": [
    "1. Dataclasses pour repr√©senter le sch√©ma normalis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84b29ebc-58fb-4090-8cf1-fe8e95a3532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColumnInfo:\n",
    "    \"\"\"\n",
    "    Repr√©sente une colonne dans une base Spider apr√®s normalisation.\n",
    "    - table_idx : index de la table dans table_names\n",
    "    - table_name : nom normalis√© de la table (minuscule)\n",
    "    - column_name : nom normalis√© de la colonne (minuscule)\n",
    "    - full_name : concat√©nation 'table.colonne'\n",
    "    \"\"\"\n",
    "    table_idx: int\n",
    "    table_name: str\n",
    "    column_name: str\n",
    "    full_name: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NormalizedSchema:\n",
    "    \"\"\"\n",
    "    Sch√©ma d'une base de donn√©es Spider apr√®s normalisation,\n",
    "    pr√™t pour le schema linking.\n",
    "    - db_id : identifiant de la base\n",
    "    - table_names : liste des noms de tables normalis√©s (minuscule)\n",
    "    - original_table_names : noms de tables originaux (pour info)\n",
    "    - columns : liste d'objets ColumnInfo\n",
    "    \"\"\"\n",
    "    db_id: str\n",
    "    table_names: List[str]\n",
    "    original_table_names: List[str]\n",
    "    columns: List[ColumnInfo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b506c71-8a1e-442d-83af-6f8b7d88c197",
   "metadata": {},
   "source": [
    "2. Fonction de normalisation du sch√©ma\n",
    "\n",
    "√Ä partir du sch√©ma brut de tables.json, on fabrique un NormalizedSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43ac1495-ec53-49eb-a5c3-eac1eeb8121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_schema(schema: Dict[str, Any]) -> NormalizedSchema:\n",
    "    \"\"\"\n",
    "    √Ä partir d'un sch√©ma Spider brut (une entr√©e de tables.json),\n",
    "    cr√©e une version normalis√©e pour le schema linking :\n",
    "      - noms de tables en minuscule\n",
    "      - noms de colonnes en minuscule\n",
    "      - construction des noms complets \"table.colonne\"\n",
    "    \"\"\"\n",
    "\n",
    "    db_id = schema[\"db_id\"]\n",
    "\n",
    "    # Noms originaux des tables (tels que dans tables.json)\n",
    "    table_names_original = schema[\"table_names\"]  # ex: [\"department\", \"head\", \"management\"]\n",
    "\n",
    "    # Version normalis√©e : tout en minuscule\n",
    "    table_names_norm = [t.lower() for t in table_names_original]\n",
    "\n",
    "    columns_info: List[ColumnInfo] = []\n",
    "\n",
    "    # schema[\"column_names\"] = liste de [table_idx, col_name]\n",
    "    for (table_idx, col_name) in schema[\"column_names\"]:\n",
    "        # table_idx = -1 correspond √† la pseudo-colonne \"*\" globale -> on l'ignore\n",
    "        if table_idx == -1:\n",
    "            continue\n",
    "\n",
    "        # nom de la table normalis√©e\n",
    "        tname = table_names_norm[table_idx]\n",
    "\n",
    "        # nom de la colonne normalis√©\n",
    "        col_norm = col_name.lower()   # ex: \"budget in billions\"\n",
    "\n",
    "        # nom complet \"table.colonne\"\n",
    "        full_name = f\"{tname}.{col_norm}\"\n",
    "\n",
    "        columns_info.append(\n",
    "            ColumnInfo(\n",
    "                table_idx=table_idx,\n",
    "                table_name=tname,\n",
    "                column_name=col_norm,\n",
    "                full_name=full_name\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return NormalizedSchema(\n",
    "        db_id=db_id,\n",
    "        table_names=table_names_norm,\n",
    "        original_table_names=table_names_original,\n",
    "        columns=columns_info\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0411ebb-11b7-4c97-a1cb-3af9779d7f0b",
   "metadata": {},
   "source": [
    "3. R√©sultat attendu pour le linking lexical\n",
    "\n",
    "On d√©finit une structure pour stocker le r√©sultat du matching lexical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c068234-8c3a-4f75-af3a-b4dcf48a0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LexicalLinkingResult:\n",
    "    \"\"\"\n",
    "    R√©sultat de la baseline de schema linking.\n",
    "    - matched_tables : liste des noms de tables d√©tect√©es dans la question\n",
    "    - matched_columns : liste des noms complets \"table.colonne\" d√©tect√©s\n",
    "    - token_to_tables : mapping idx_token -> tables candidates\n",
    "    - token_to_columns : mapping idx_token -> colonnes candidates\n",
    "    \"\"\"\n",
    "    matched_tables: List[str]\n",
    "    matched_columns: List[str]\n",
    "    token_to_tables: Dict[int, List[str]]\n",
    "    token_to_columns: Dict[int, List[str]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7a4ff-4e2c-4d37-8700-71ff837be13a",
   "metadata": {},
   "source": [
    "4. Fonction de baseline : matching lexical simple\n",
    "Ici on utilise les lemmes des tokens de la question (√©tape 3) et on les compare aux noms de tables / colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c28fb596-04d5-4d7a-a60a-4b4ba3bbdc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_schema_linking(\n",
    "    q_preproc: QuestionPreproc,\n",
    "    norm_schema: NormalizedSchema\n",
    ") -> LexicalLinkingResult:\n",
    "    \"\"\"\n",
    "    Baseline de schema linking bas√©e sur le matching lexical :\n",
    "    - Pour chaque token (lemme) de la question :\n",
    "        * on cherche s'il correspond √† un nom de table\n",
    "        * on cherche s'il correspond (ou appara√Æt) dans un nom de colonne\n",
    "    - On accumule les tables/colonnes \"match√©es\" et on garde\n",
    "      une trace de quels tokens pointent vers quelles tables/colonnes.\n",
    "    \"\"\"\n",
    "\n",
    "    matched_tables = set()    # ensemble de noms de tables\n",
    "    matched_columns = set()   # ensemble de noms \"table.colonne\"\n",
    "\n",
    "    token_to_tables: Dict[int, List[str]] = {}\n",
    "    token_to_columns: Dict[int, List[str]] = {}\n",
    "\n",
    "    # Parcours des tokens de la question pr√©trait√©e\n",
    "    for tok in q_preproc.tokens:\n",
    "        # On travaille avec le lemme en minuscule\n",
    "        lemma = tok.lemma.lower()\n",
    "\n",
    "        # On ignore les tokens vides ou tr√®s courts (ex: \"a\", \"of\"... √©ventuellement)\n",
    "        if len(lemma) < 2:\n",
    "            continue\n",
    "\n",
    "        # ================\n",
    "        # 1) Matching avec les tables\n",
    "        # ================\n",
    "        for tname in norm_schema.table_names:\n",
    "            # Matching tr√®s simple :\n",
    "            # - √©galit√© exacte (lemma == nom de table)\n",
    "            # - inclusion (lemma dans tname ou l'inverse)\n",
    "            if lemma == tname:\n",
    "                matched_tables.add(tname)\n",
    "                token_to_tables.setdefault(tok.idx, []).append(tname)\n",
    "            elif lemma in tname or tname in lemma:\n",
    "                matched_tables.add(tname)\n",
    "                token_to_tables.setdefault(tok.idx, []).append(tname)\n",
    "\n",
    "        # ================\n",
    "        # 2) Matching avec les colonnes\n",
    "        # ================\n",
    "        for col in norm_schema.columns:\n",
    "            # On va d√©couper le nom de la colonne en mots\n",
    "            # ex: \"budget in billions\" -> [\"budget\", \"in\", \"billions\"]\n",
    "            col_tokens = col.column_name.split()\n",
    "\n",
    "            # Matching si le lemme est exactement un des mots de la colonne\n",
    "            # ou si le lemme est √©gal au nom complet de la colonne.\n",
    "            if lemma == col.column_name or lemma in col_tokens:\n",
    "                matched_columns.add(col.full_name)\n",
    "                token_to_columns.setdefault(tok.idx, []).append(col.full_name)\n",
    "\n",
    "    return LexicalLinkingResult(\n",
    "        matched_tables=sorted(matched_tables),\n",
    "        matched_columns=sorted(matched_columns),\n",
    "        token_to_tables=token_to_tables,\n",
    "        token_to_columns=token_to_columns\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5404c-4b9d-4b15-8d67-5b2eb7847fd1",
   "metadata": {},
   "source": [
    "5. Exemple complet sur ton premier exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2efa0c93-c7d6-41e6-901c-030c97e05af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question originale : How many heads of the departments are older than 56 ?\n",
      "db_id : department_management\n",
      "\n",
      "=== R√©sultat baseline Schema Linking (√âtape 4) ===\n",
      "Tables d√©tect√©es : ['department', 'head']\n",
      "Colonnes d√©tect√©es :\n",
      "  - department.department id\n",
      "  - head.head id\n",
      "  - management.department id\n",
      "  - management.head id\n",
      "\n",
      "D√©tails token -> tables :\n",
      "  token 'heads' (idx=2) ‚Üí tables : ['head']\n",
      "  token 'departments' (idx=5) ‚Üí tables : ['department']\n",
      "\n",
      "D√©tails token -> colonnes :\n",
      "  token 'heads' (idx=2) ‚Üí colonnes : ['head.head id', 'management.head id']\n",
      "  token 'departments' (idx=5) ‚Üí colonnes : ['department.department id', 'management.department id']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Exemple d'utilisation ‚Äì √âtape 4\n",
    "# ============================================\n",
    "\n",
    "example = train_data[0]\n",
    "\n",
    "question = example[\"question\"]\n",
    "db_id = example[\"db_id\"]\n",
    "\n",
    "print(\"Question originale :\", question)\n",
    "print(\"db_id :\", db_id)\n",
    "\n",
    "# 1) R√©cup√©rer le sch√©ma brut\n",
    "schema_raw = get_db_schema(db_id, tables)\n",
    "\n",
    "# 2) Normaliser le sch√©ma\n",
    "norm_schema = normalize_schema(schema_raw)\n",
    "\n",
    "# 3) Pr√©traiter la question (√âtape 3 d√©j√† faite)\n",
    "q_preproc = preprocess_question(question, nlp)\n",
    "\n",
    "# 4) Appliquer la baseline de schema linking\n",
    "lex_links = lexical_schema_linking(q_preproc, norm_schema)\n",
    "\n",
    "print(\"\\n=== R√©sultat baseline Schema Linking (√âtape 4) ===\")\n",
    "print(\"Tables d√©tect√©es :\", lex_links.matched_tables)\n",
    "print(\"Colonnes d√©tect√©es :\")\n",
    "for col in lex_links.matched_columns:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "print(\"\\nD√©tails token -> tables :\")\n",
    "for idx, tables_list in lex_links.token_to_tables.items():\n",
    "    tok = q_preproc.tokens[idx]\n",
    "    print(f\"  token '{tok.text}' (idx={idx}) ‚Üí tables : {tables_list}\")\n",
    "\n",
    "print(\"\\nD√©tails token -> colonnes :\")\n",
    "for idx, cols_list in lex_links.token_to_columns.items():\n",
    "    tok = q_preproc.tokens[idx]\n",
    "    print(f\"  token '{tok.text}' (idx={idx}) ‚Üí colonnes : {cols_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "087290b0-5f60-4247-9684-ce0f534739ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question originale : List the name, born state and age of the heads of departments ordered by age.\n",
      "db_id : department_management\n",
      "\n",
      "Question normalis√©e : list the name born state and age of the heads of departments ordered by age\n",
      "Tokens :\n",
      "  idx= 0 text=list       lemma=list       pos=VERB  dep=ROOT       head=list\n",
      "  idx= 1 text=the        lemma=the        pos=DET   dep=det        head=name\n",
      "  idx= 2 text=name       lemma=name       pos=NOUN  dep=npadvmod   head=born\n",
      "  idx= 3 text=born       lemma=bear       pos=VERB  dep=amod       head=state\n",
      "  idx= 4 text=state      lemma=state      pos=NOUN  dep=dobj       head=list\n",
      "  idx= 5 text=and        lemma=and        pos=CCONJ dep=cc         head=state\n",
      "  idx= 6 text=age        lemma=age        pos=NOUN  dep=conj       head=state\n",
      "  idx= 7 text=of         lemma=of         pos=ADP   dep=prep       head=state\n",
      "  idx= 8 text=the        lemma=the        pos=DET   dep=det        head=heads\n",
      "  idx= 9 text=heads      lemma=head       pos=NOUN  dep=pobj       head=of\n",
      "  idx=10 text=of         lemma=of         pos=ADP   dep=prep       head=heads\n",
      "  idx=11 text=departments lemma=department pos=NOUN  dep=pobj       head=of\n",
      "  idx=12 text=ordered    lemma=order      pos=VERB  dep=acl        head=departments\n",
      "  idx=13 text=by         lemma=by         pos=ADP   dep=agent      head=ordered\n",
      "  idx=14 text=age        lemma=age        pos=NOUN  dep=pobj       head=by\n",
      "\n",
      "Valeurs candidates : []\n",
      "\n",
      "=== R√©sultat baseline Schema Linking pour cet exemple ===\n",
      "Tables d√©tect√©es : ['department', 'head', 'management']\n",
      "Colonnes d√©tect√©es :\n",
      "  - department.department id\n",
      "  - department.name\n",
      "  - head.age\n",
      "  - head.born state\n",
      "  - head.head id\n",
      "  - head.name\n",
      "  - management.department id\n",
      "  - management.head id\n",
      "\n",
      "D√©tails token -> tables :\n",
      "  token 'age' (idx=6) ‚Üí tables : ['management']\n",
      "  token 'heads' (idx=9) ‚Üí tables : ['head']\n",
      "  token 'departments' (idx=11) ‚Üí tables : ['department']\n",
      "  token 'age' (idx=14) ‚Üí tables : ['management']\n",
      "\n",
      "D√©tails token -> colonnes :\n",
      "  token 'name' (idx=2) ‚Üí colonnes : ['department.name', 'head.name']\n",
      "  token 'state' (idx=4) ‚Üí colonnes : ['head.born state']\n",
      "  token 'age' (idx=6) ‚Üí colonnes : ['head.age']\n",
      "  token 'heads' (idx=9) ‚Üí colonnes : ['head.head id', 'management.head id']\n",
      "  token 'departments' (idx=11) ‚Üí colonnes : ['department.department id', 'management.department id']\n",
      "  token 'age' (idx=14) ‚Üí colonnes : ['head.age']\n"
     ]
    }
   ],
   "source": [
    "# On prend le 2·µâ exemple (index 1) de train_spider\n",
    "example2 = train_data[1]\n",
    "\n",
    "question2 = example2[\"question\"]\n",
    "db_id2 = example2[\"db_id\"]\n",
    "\n",
    "print(\"Question originale :\", question2)\n",
    "print(\"db_id :\", db_id2)\n",
    "\n",
    "# 1) R√©cup√©rer le sch√©ma brut\n",
    "schema_raw2 = get_db_schema(db_id2, tables)\n",
    "\n",
    "# 2) Normaliser le sch√©ma\n",
    "norm_schema2 = normalize_schema(schema_raw2)\n",
    "\n",
    "# 3) Pr√©traiter la question (√âtape 3)\n",
    "q_preproc2 = preprocess_question(question2, nlp)\n",
    "\n",
    "print(\"\\nQuestion normalis√©e :\", q_preproc2.normalized)\n",
    "print(\"Tokens :\")\n",
    "for t in q_preproc2.tokens:\n",
    "    print(f\"  idx={t.idx:2d} text={t.text:10s} lemma={t.lemma:10s} pos={t.pos:5s} dep={t.dep:10s} head={t.head}\")\n",
    "\n",
    "print(\"\\nValeurs candidates :\", q_preproc2.values)\n",
    "\n",
    "# 4) Appliquer la baseline de schema linking (√âtape 4)\n",
    "lex_links2 = lexical_schema_linking(q_preproc2, norm_schema2)\n",
    "\n",
    "print(\"\\n=== R√©sultat baseline Schema Linking pour cet exemple ===\")\n",
    "print(\"Tables d√©tect√©es :\", lex_links2.matched_tables)\n",
    "print(\"Colonnes d√©tect√©es :\")\n",
    "for col in lex_links2.matched_columns:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "print(\"\\nD√©tails token -> tables :\")\n",
    "for idx, tables_list in lex_links2.token_to_tables.items():\n",
    "    tok = q_preproc2.tokens[idx]\n",
    "    print(f\"  token '{tok.text}' (idx={idx}) ‚Üí tables : {tables_list}\")\n",
    "\n",
    "print(\"\\nD√©tails token -> colonnes :\")\n",
    "for idx, cols_list in lex_links2.token_to_columns.items():\n",
    "    tok = q_preproc2.tokens[idx]\n",
    "    print(f\"  token '{tok.text}' (idx={idx}) ‚Üí colonnes : {cols_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b388fe-bb2a-4cb2-99f2-62ddaa2b20dd",
   "metadata": {},
   "source": [
    "Dans l‚Äô√©tape 4, nous avons mis en place une baseline de schema linking bas√©e uniquement sur le matching lexical.\n",
    "Pour chaque token de la question (via son lemme), nous comparons son contenu aux noms normalis√©s des tables et colonnes du sch√©ma afin d‚Äôidentifier les √©l√©ments susceptibles d‚Äô√™tre mentionn√©s dans la question.\n",
    "Cette √©tape fournit une premi√®re approximation des tables et colonnes pertinentes, servant de fondation aux m√©thodes plus avanc√©es du schema linking dans l‚Äô√©tape 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78652617-5704-4d46-af1e-2b49cae775b9",
   "metadata": {},
   "source": [
    "Bien que l‚Äô√©tape 4 permette d‚Äôidentifier des correspondances simples entre les lemmes de la question et les √©l√©ments du sch√©ma, cette approche reste limit√©e face aux variations lexicales. L‚Äô√©tape 5 vient alors enrichir ce processus gr√¢ce √† un NER schema-aware combinant analyse linguistique et fuzzy matching, permettant de d√©tecter plus pr√©cis√©ment les tables, colonnes et valeurs r√©ellement mentionn√©es dans la question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2516448-f9b1-4de2-b04c-71f861f60728",
   "metadata": {},
   "source": [
    "### √âtape 5 : NER schema-aware avanc√© avec spaCy + fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64d15178-7f24-4e41-bf40-9c038c686b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SchemaEntity:\n",
    "    \"\"\"\n",
    "    Repr√©sente une entit√© d√©tect√©e dans la question et li√©e au sch√©ma.\n",
    "\n",
    "    - label : type d'entit√© (ex: \"TABLE\", \"COLUMN\", \"VALUE\")\n",
    "    - text : texte tel qu'il appara√Æt dans la question\n",
    "    - start_idx : index du token de d√©but (dans la question spaCy)\n",
    "    - end_idx : index du token de fin (exclu)\n",
    "    - linked_schema : nom de table / colonne correspondante dans le sch√©ma\n",
    "                      (ex: \"head\" ou \"head.age\"), ou None pour VALUE brute\n",
    "    - score : score de similarit√© / confiance (entre 0 et 1)\n",
    "    \"\"\"\n",
    "    label: str\n",
    "    text: str\n",
    "    start_idx: int\n",
    "    end_idx: int\n",
    "    linked_schema: Optional[str]\n",
    "    score: float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beada6a7-542f-4451-988b-389dd948f385",
   "metadata": {},
   "source": [
    "Petite fonction utilitaire : meilleur match fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d67d4d50-bd30-4bf8-ace3-0c1c7065eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_best_match(cand: str, choices: List[str]) -> Tuple[Optional[str], float]:\n",
    "    \"\"\"\n",
    "    Retourne la meilleure correspondance fuzzy (approx.) entre une cha√Æne 'cand'\n",
    "    et une liste de 'choices' (noms de tables ou colonnes).\n",
    "\n",
    "    On utilise rapidfuzz.fuzz.ratio, qui donne un score entre 0 et 100.\n",
    "    On renvoie (√©l√©ment_choisi, score).\n",
    "    \"\"\"\n",
    "    best_item = None\n",
    "    best_score = 0.0\n",
    "\n",
    "    for ch in choices:\n",
    "        s = fuzz.ratio(cand, ch)  # similarit√© de 0 √† 100\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_item = ch\n",
    "\n",
    "    return best_item, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd216e26-44e6-4bc0-a208-2e71cfcca423",
   "metadata": {},
   "source": [
    "Fonction principale : NER schema-aware\n",
    "\n",
    "Id√©e :\n",
    "\n",
    "On r√©utilise les valeurs d√©j√† extraites dans l‚Äô√©tape 3 (q_preproc.values) ‚Üí entit√©s VALUE\n",
    "\n",
    "Pour chaque nom (NOUN) ou nom propre (PROPN) de la question :\n",
    "\n",
    "on cherche le meilleur match fuzzy parmi les tables\n",
    "\n",
    "et parmi les colonnes\n",
    "\n",
    "selon les scores, on d√©cide si c‚Äôest plut√¥t une TABLE, une COLUMN, ou rien\n",
    "\n",
    "Tu peux ajuster les seuils (table_threshold, column_threshold) en fonction de tes tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58b83459-080d-47bc-b35b-3c15c9273be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_aware_ner(\n",
    "    q_preproc: QuestionPreproc,\n",
    "    norm_schema: NormalizedSchema,\n",
    "    table_threshold: int = 80,\n",
    "    column_threshold: int = 70\n",
    ") -> List[SchemaEntity]:\n",
    "    \"\"\"\n",
    "    D√©tecte des entit√©s orient√©es sch√©ma dans la question :\n",
    "      - TABLE : token qui ressemble √† un nom de table\n",
    "      - COLUMN : token qui ressemble √† un nom de colonne (ou partie de colonne)\n",
    "      - VALUE : valeurs (nombres, strings) d√©j√† extraites √† l'√©tape 3\n",
    "\n",
    "    Combinaison de :\n",
    "      - analyse linguistique (POS, lemmes) de spaCy\n",
    "      - fuzzy matching (rapidfuzz) avec les noms de tables/colonnes\n",
    "    \"\"\"\n",
    "\n",
    "    entities: List[SchemaEntity] = []\n",
    "    doc = q_preproc.doc  # doc spaCy issu de preprocess_question\n",
    "\n",
    "    # ===========================\n",
    "    # 1) Entit√©s de type VALUE\n",
    "    # ===========================\n",
    "    for val in q_preproc.values:\n",
    "        # On essaie de retrouver la position de cette valeur dans les tokens\n",
    "        start_idx = -1\n",
    "        for tok in q_preproc.tokens:\n",
    "            if tok.text == val:\n",
    "                start_idx = tok.idx\n",
    "                break\n",
    "\n",
    "        entities.append(\n",
    "            SchemaEntity(\n",
    "                label=\"VALUE\",\n",
    "                text=val,\n",
    "                start_idx=start_idx,\n",
    "                end_idx=start_idx + 1 if start_idx >= 0 else -1,\n",
    "                linked_schema=None,   # pas encore li√©e √† une colonne pr√©cise\n",
    "                score=1.0             # confiance max : on a vu la valeur clairement\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ===========================\n",
    "    # 2) Entit√©s TABLE / COLUMN\n",
    "    # ===========================\n",
    "\n",
    "    # Liste des noms de tables (ex: [\"department\", \"head\", \"management\"])\n",
    "    table_candidates = norm_schema.table_names\n",
    "\n",
    "    # Construction d'une liste enrichie de candidats colonnes\n",
    "    # + d'une map pour retrouver le full_name (\"table.colonne\")\n",
    "    column_candidates: List[str] = []\n",
    "    column_map: Dict[str, str] = {}\n",
    "\n",
    "    for c in norm_schema.columns:\n",
    "        # 1) full name \"table.colonne\"\n",
    "        column_candidates.append(c.full_name)\n",
    "        column_map[c.full_name] = c.full_name\n",
    "\n",
    "        # 2) nom complet de la colonne (ex: \"budget in billions\")\n",
    "        column_candidates.append(c.column_name)\n",
    "        column_map[c.column_name] = c.full_name\n",
    "\n",
    "        # 3) mots individuels (ex: \"budget\", \"billions\")\n",
    "        for piece in c.column_name.split():\n",
    "            column_candidates.append(piece)\n",
    "            column_map[piece] = c.full_name\n",
    "\n",
    "    # On parcourt les tokens spaCy (doc) plut√¥t que q_preproc.tokens\n",
    "    # pour avoir acc√®s aux infos POS/dep compl√®tes\n",
    "    for tok in doc:\n",
    "        # On ne s'int√©resse qu'aux noms (NOUN) et noms propres (PROPN)\n",
    "        if tok.pos_ not in (\"NOUN\", \"PROPN\",\"VERB\"):\n",
    "            continue\n",
    "\n",
    "        lemma = tok.lemma_.lower()\n",
    "\n",
    "        # On ignore les tout petits tokens (comme \"a\", \"an\")\n",
    "        if len(lemma) < 2:\n",
    "            continue\n",
    "\n",
    "        # Meilleur match fuzzy avec les tables\n",
    "        best_table, score_table = fuzzy_best_match(lemma, table_candidates)\n",
    "\n",
    "        # Meilleur match fuzzy avec les colonnes ENRICHI\n",
    "        best_column_key, score_column = fuzzy_best_match(lemma, column_candidates)\n",
    "\n",
    "        # ===============================\n",
    "        # D√©cision : TABLE vs COLUMN\n",
    "        # ===============================\n",
    "\n",
    "        # 1) TABLE si score bon et >= score colonne\n",
    "        if best_table is not None and score_table >= table_threshold and score_table >= score_column:\n",
    "            ent = SchemaEntity(\n",
    "                label=\"TABLE\",\n",
    "                text=tok.text,\n",
    "                start_idx=tok.i,\n",
    "                end_idx=tok.i + 1,\n",
    "                linked_schema=best_table,\n",
    "                score=score_table / 100.0\n",
    "            )\n",
    "            entities.append(ent)\n",
    "\n",
    "        # 2) sinon COLUMN si score colonne assez bon\n",
    "        elif best_column_key is not None and score_column >= column_threshold:\n",
    "            full_name = column_map[best_column_key]  # r√©cup√®re \"table.colonne\"\n",
    "            ent = SchemaEntity(\n",
    "                label=\"COLUMN\",\n",
    "                text=tok.text,\n",
    "                start_idx=tok.i,\n",
    "                end_idx=tok.i + 1,\n",
    "                linked_schema=full_name,\n",
    "                score=score_column / 100.0\n",
    "            )\n",
    "            entities.append(ent)\n",
    "\n",
    "        # Sinon, on ignore ce token (pas suffisamment ressemblant)\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b6cda1f-0df5-4ac5-8a05-e7db27abf317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question originale : How many heads of the departments are older than 56 ?\n",
      "db_id : department_management\n",
      "\n",
      "=== Entit√©s schema-aware d√©tect√©es (√âtape 5) ===\n",
      "- [VALUE] '56' (tokens 9-10) ‚Üí linked_schema=None, score=1.00\n",
      "- [TABLE] 'heads' (tokens 2-3) ‚Üí linked_schema=head, score=1.00\n",
      "- [TABLE] 'departments' (tokens 5-6) ‚Üí linked_schema=department, score=1.00\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Exemple d'utilisation √âtape 5\n",
    "# ===========================\n",
    "\n",
    "example = train_data[0]\n",
    "question = example[\"question\"]\n",
    "db_id = example[\"db_id\"]\n",
    "\n",
    "print(\"Question originale :\", question)\n",
    "print(\"db_id :\", db_id)\n",
    "\n",
    "# Sch√©ma brut + normalisation\n",
    "schema_raw = get_db_schema(db_id, tables)\n",
    "norm_schema = normalize_schema(schema_raw)\n",
    "\n",
    "# Pr√©processing (√âtape 3)\n",
    "q_preproc = preprocess_question(question, nlp)\n",
    "\n",
    "# NER schema-aware (√âtape 5)\n",
    "schema_entities = schema_aware_ner(q_preproc, norm_schema)\n",
    "\n",
    "print(\"\\n=== Entit√©s schema-aware d√©tect√©es (√âtape 5) ===\")\n",
    "for ent in schema_entities:\n",
    "    print(f\"- [{ent.label}] '{ent.text}' (tokens {ent.start_idx}-{ent.end_idx}) \"\n",
    "          f\"‚Üí linked_schema={ent.linked_schema}, score={ent.score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51438c9f-27db-42f8-923c-204811e13eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question originale : List the name and age of the heads in departments with budget in billions greater than 10\n",
      "Question normalis√©e : list the name and age of the heads in departments with budget in billions greater than 10\n",
      "\n",
      "=== Entit√©s schema-aware d√©tect√©es (√âtape 5) ===\n",
      "- [VALUE] '10' (tokens 16-17) ‚Üí linked_schema=None, score=1.00\n",
      "- [COLUMN] 'name' (tokens 2-3) ‚Üí linked_schema=head.name, score=1.00\n",
      "- [COLUMN] 'age' (tokens 4-5) ‚Üí linked_schema=head.age, score=1.00\n",
      "- [TABLE] 'heads' (tokens 7-8) ‚Üí linked_schema=head, score=1.00\n",
      "- [TABLE] 'departments' (tokens 9-10) ‚Üí linked_schema=department, score=1.00\n",
      "- [COLUMN] 'budget' (tokens 11-12) ‚Üí linked_schema=department.budget in billions, score=1.00\n",
      "- [COLUMN] 'billions' (tokens 13-14) ‚Üí linked_schema=department.budget in billions, score=0.93\n"
     ]
    }
   ],
   "source": [
    "question_test = \"List the name and age of the heads in departments with budget in billions greater than 10\"\n",
    "db_id_test = \"department_management\"\n",
    "schema_raw = get_db_schema(db_id_test, tables)\n",
    "norm_schema = normalize_schema(schema_raw)\n",
    "q_preproc_test = preprocess_question(question_test, nlp)\n",
    "schema_entities_test = schema_aware_ner(q_preproc_test, norm_schema)\n",
    "\n",
    "print(\"Question originale :\", question_test)\n",
    "print(\"Question normalis√©e :\", q_preproc_test.normalized)\n",
    "print(\"\\n=== Entit√©s schema-aware d√©tect√©es (√âtape 5) ===\")\n",
    "for ent in schema_entities_test:\n",
    "    print(f\"- [{ent.label}] '{ent.text}' (tokens {ent.start_idx}-{ent.end_idx}) \"\n",
    "          f\"‚Üí linked_schema={ent.linked_schema}, score={ent.score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c4f492f-fc9f-4811-a0a3-c6996027c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question originale : List all departments created before the year 1990.\n",
      "Question normalis√©e : list all departments created before the year 1990\n",
      "\n",
      "=== Entit√©s schema-aware d√©tect√©es (√âtape 5) ===\n",
      "- [VALUE] '1990' (tokens 7-8) ‚Üí linked_schema=None, score=1.00\n",
      "- [TABLE] 'departments' (tokens 2-3) ‚Üí linked_schema=department, score=1.00\n",
      "- [COLUMN] 'created' (tokens 3-4) ‚Üí linked_schema=department.creation, score=0.71\n"
     ]
    }
   ],
   "source": [
    "question_test = \"List all departments created before the year 1990.\"\n",
    "db_id_test = \"department_management\"\n",
    "schema_raw = get_db_schema(db_id_test, tables)\n",
    "norm_schema = normalize_schema(schema_raw)\n",
    "q_preproc_test = preprocess_question(question_test, nlp)\n",
    "schema_entities_test = schema_aware_ner(q_preproc_test, norm_schema)\n",
    "\n",
    "print(\"Question originale :\", question_test)\n",
    "print(\"Question normalis√©e :\", q_preproc_test.normalized)\n",
    "print(\"\\n=== Entit√©s schema-aware d√©tect√©es (√âtape 5) ===\")\n",
    "for ent in schema_entities_test:\n",
    "    print(f\"- [{ent.label}] '{ent.text}' (tokens {ent.start_idx}-{ent.end_idx}) \"\n",
    "          f\"‚Üí linked_schema={ent.linked_schema}, score={ent.score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129e451-ce5a-43e5-9823-a17428b99c8d",
   "metadata": {},
   "source": [
    "L‚Äô√©tape 5 r√©alise un rep√©rage intelligent des √©l√©ments du sch√©ma mentionn√©s dans la question (tables, colonnes et valeurs), en reliant les mots du langage naturel aux structures exactes de la base de donn√©es. Cela permet d‚Äôidentifier quelles parties du sch√©ma sont r√©ellement pertinentes pour r√©pondre √† la question. Ces informations serviront ensuite √† construire un input textuel riche et pr√©cis, qui guidera efficacement le mod√®le g√©n√©ratif (comme BART) dans la production de la requ√™te SQL correcte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce06360-4874-40fc-90d4-515bd71f28da",
   "metadata": {},
   "source": [
    "### √âtape 6 ‚Äî Pr√©paration et Structuration du Contexte Sch√©ma‚ÄìQuestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9560aa-b83b-4d42-9508-42c715129ef6",
   "metadata": {},
   "source": [
    "Dans cette √©tape, nous rassemblons et organisons toutes les informations extraites des √©tapes pr√©c√©dentes afin de pr√©parer la construction de l‚Äôentr√©e textuelle destin√©e au mod√®le g√©n√©ratif. Concr√®tement, l‚Äô√©tape 6 regroupe :\n",
    "\n",
    "la question pr√©trait√©e (lemmatisation, valeurs num√©riques, tokens utiles) ;\n",
    "\n",
    "le sch√©ma normalis√© associ√© √† la base cible (tables, colonnes et cl√©s √©trang√®res) ;\n",
    "\n",
    "les entit√©s orient√©es sch√©ma identifi√©es √† l‚Äô√©tape 5 (tables, colonnes et valeurs pertinentes).\n",
    "\n",
    "Cette structuration a pour objectif de filtrer et de mettre en forme uniquement les √©l√©ments du sch√©ma r√©ellement pertinents pour la g√©n√©ration SQL. L‚Äô√©tape 6 agit ainsi comme une couche d‚Äôint√©gration, assurant la transition entre l‚Äôanalyse linguistique/s√©mantique (√©tapes 3 √† 5) et la construction de l‚Äôinput final pour le mod√®le Seq2Seq (√©tape 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dd4f0c06-1856-470c-9542-26da66dae64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelContext:\n",
    "    \"\"\"\n",
    "    Repr√©sente le contexte complet n√©cessaire pour la g√©n√©ration SQL :\n",
    "      - question originale et pr√©trait√©e\n",
    "      - sch√©ma brut et normalis√©\n",
    "      - entit√©s orient√©es sch√©ma (TABLE, COLUMN, VALUE)\n",
    "    \"\"\"\n",
    "    db_id: str\n",
    "    question_original: str\n",
    "    question_preproc: QuestionPreproc\n",
    "    schema_raw: Dict[str, Any]\n",
    "    schema_norm: NormalizedSchema\n",
    "    schema_entities: List[SchemaEntity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03c6f18c-62c6-4182-a793-5a5057119183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_context(\n",
    "    example: Dict[str, Any],\n",
    "    tables: List[Dict[str, Any]],\n",
    "    nlp\n",
    ") -> ModelContext:\n",
    "    \"\"\"\n",
    "    √âtape 6 : pr√©paration et structuration du contexte sch√©ma‚Äìquestion.\n",
    "\n",
    "    √Ä partir d'un exemple du dataset Spider, cette fonction :\n",
    "      1) r√©cup√®re le sch√©ma brut correspondant (tables.json),\n",
    "      2) normalise ce sch√©ma (tables, colonnes, types, foreign keys),\n",
    "      3) pr√©traite la question (√âtape 3),\n",
    "      4) applique le NER orient√© sch√©ma (√âtape 5).\n",
    "\n",
    "    Elle renvoie un objet ModelContext qui regroupe toutes ces informations\n",
    "    et qui servira d'entr√©e √† l'√©tape 7 (construction de l'input texte\n",
    "    pour BART).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) R√©cup√©ration de la question et de l'identifiant de la DB\n",
    "    question = example[\"question\"]\n",
    "    db_id = example[\"db_id\"]\n",
    "\n",
    "    # 2) Sch√©ma brut √† partir de tables.json\n",
    "    schema_raw = get_db_schema(db_id, tables)\n",
    "\n",
    "    # 3) Normalisation du sch√©ma (noms de tables/colonnes, etc.)\n",
    "    schema_norm = normalize_schema(schema_raw)\n",
    "\n",
    "    # 4) Pr√©traitement de la question (√âtape 3)\n",
    "    q_preproc = preprocess_question(question, nlp)\n",
    "\n",
    "    # 5) NER schema-aware (√âtape 5)\n",
    "    schema_entities = schema_aware_ner(q_preproc, schema_norm)\n",
    "\n",
    "    # 6) Construction du contexte global\n",
    "    ctx = ModelContext(\n",
    "        db_id=db_id,\n",
    "        question_original=question,\n",
    "        question_preproc=q_preproc,\n",
    "        schema_raw=schema_raw,\n",
    "        schema_norm=schema_norm,\n",
    "        schema_entities=schema_entities,\n",
    "    )\n",
    "\n",
    "    return ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "34df1456-6225-4ecf-8034-78a79c2d4030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB : department_management\n",
      "Question originale : How many heads of the departments are older than 56 ?\n",
      "Question normalis√©e : how many heads of the departments are older than 56\n",
      "\n",
      "Tables normalis√©es : ['department', 'head', 'management']\n",
      "\n",
      "Entit√©s d√©tect√©es :\n",
      "- VALUE '56' -> None, score=1.00\n",
      "- TABLE 'heads' -> head, score=1.00\n",
      "- TABLE 'departments' -> department, score=1.00\n"
     ]
    }
   ],
   "source": [
    "# On prend un exemple du train Spider\n",
    "example = train_data[0]\n",
    "\n",
    "# √âtape 6 : on construit le contexte\n",
    "ctx = build_model_context(example, tables, nlp)\n",
    "\n",
    "print(\"DB :\", ctx.db_id)\n",
    "print(\"Question originale :\", ctx.question_original)\n",
    "print(\"Question normalis√©e :\", ctx.question_preproc.normalized)\n",
    "\n",
    "print(\"\\nTables normalis√©es :\", ctx.schema_norm.table_names)\n",
    "\n",
    "print(\"\\nEntit√©s d√©tect√©es :\")\n",
    "for ent in ctx.schema_entities:\n",
    "    print(f\"- {ent.label} '{ent.text}' -> {ent.linked_schema}, score={ent.score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bd053adc-1de1-4dbc-ab1d-63fca184ade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def build_model_input(\n",
    "    question: str,\n",
    "    db_id: str,\n",
    "    q_preproc: QuestionPreproc,\n",
    "    norm_schema: NormalizedSchema,\n",
    "    schema_entities: List[SchemaEntity],\n",
    "    schema_raw: Dict[str, Any],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    √âtape 7 : lin√©arise la question + le sch√©ma en un texte structur√©\n",
    "    utilisable comme entr√©e pour un mod√®le Seq2Seq (ex: BART).\n",
    "\n",
    "    On inclut :\n",
    "      - la question normalis√©e,\n",
    "      - les tables + colonnes,\n",
    "      - les foreign keys (cas g√©n√©ral),\n",
    "      - les entit√©s schema-aware (TABLE, COLUMN, VALUE).\n",
    "    \"\"\"\n",
    "\n",
    "    lines: List[str] = []\n",
    "\n",
    "    # 1) Infos g√©n√©rales\n",
    "    lines.append(f\"DB: {db_id}\")\n",
    "    lines.append(f\"QUESTION: {q_preproc.normalized}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # 2) Tables + colonnes\n",
    "    lines.append(\"TABLES:\")\n",
    "    for i, tname in enumerate(norm_schema.table_names):\n",
    "        cols = [c.column_name for c in norm_schema.columns if c.table_idx == i]\n",
    "        cols_str = \", \".join(cols) if cols else \"(no columns)\"\n",
    "        lines.append(f\"- {tname}: {cols_str}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # 3) Foreign keys (relations entre colonnes/tables)\n",
    "    fk_list = schema_raw.get(\"foreign_keys\", [])\n",
    "    col_names_raw = schema_raw.get(\"column_names\", [])\n",
    "    table_names_raw = schema_raw.get(\"table_names\", [])\n",
    "\n",
    "    if fk_list:\n",
    "        lines.append(\"FOREIGN_KEYS:\")\n",
    "        for src_idx, tgt_idx in fk_list:\n",
    "            if not (0 <= src_idx < len(col_names_raw) and 0 <= tgt_idx < len(col_names_raw)):\n",
    "                continue\n",
    "\n",
    "            src_table_idx, src_col_name = col_names_raw[src_idx]\n",
    "            tgt_table_idx, tgt_col_name = col_names_raw[tgt_idx]\n",
    "\n",
    "            if src_table_idx == -1 or tgt_table_idx == -1:\n",
    "                continue\n",
    "\n",
    "            src_table_name = table_names_raw[src_table_idx]\n",
    "            tgt_table_name = table_names_raw[tgt_table_idx]\n",
    "\n",
    "            src_full = f\"{src_table_name}.{src_col_name}\"\n",
    "            tgt_full = f\"{tgt_table_name}.{tgt_col_name}\"\n",
    "\n",
    "            lines.append(f\"- {src_full} -> {tgt_full}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    # 4) Entit√©s schema-aware (TABLE / COLUMN / VALUE)\n",
    "    lines.append(\"ENTITIES:\")\n",
    "    for ent in schema_entities:\n",
    "        lines.append(f\"- {ent.label} '{ent.text}' -> {ent.linked_schema}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9f8febaa-922b-4901-bb64-8446f5d69f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== INPUT POUR BART =====\n",
      "DB: department_management\n",
      "QUESTION: how many heads of the departments are older than 56\n",
      "\n",
      "TABLES:\n",
      "- department: department id, name, creation, ranking, budget in billions, num employees\n",
      "- head: head id, name, born state, age\n",
      "- management: department id, head id, temporary acting\n",
      "\n",
      "FOREIGN_KEYS:\n",
      "- management.head id -> head.head id\n",
      "- management.department id -> department.department id\n",
      "\n",
      "ENTITIES:\n",
      "- VALUE '56' -> None\n",
      "- TABLE 'heads' -> head\n",
      "- TABLE 'departments' -> department\n",
      "\n",
      "\n",
      "===== SQL CIBLE (Spider) =====\n",
      "SELECT count(*) FROM head WHERE age  >  56\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Test √âtapes 6 + 7 sur un exemple\n",
    "# ===========================\n",
    "\n",
    "example = train_data[0]\n",
    "\n",
    "# √âtape 6 : construire le contexte\n",
    "ctx = build_model_context(example, tables, nlp)\n",
    "\n",
    "# √âtape 7 : construire l'input texte pour BART\n",
    "model_input = build_model_input(\n",
    "    question=ctx.question_original,\n",
    "    db_id=ctx.db_id,\n",
    "    q_preproc=ctx.question_preproc,\n",
    "    norm_schema=ctx.schema_norm,\n",
    "    schema_entities=ctx.schema_entities,\n",
    "    schema_raw=ctx.schema_raw,\n",
    ")\n",
    "\n",
    "print(\"===== INPUT POUR BART =====\")\n",
    "print(model_input)\n",
    "print(\"\\n===== SQL CIBLE (Spider) =====\")\n",
    "print(example[\"query\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e80299f8-8c7e-43b7-8a6f-ec2cbc059329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== INPUT POUR BART (question_test) =====\n",
      "DB: department_management\n",
      "QUESTION: list all departments created before the year 1990\n",
      "\n",
      "TABLES:\n",
      "- department: department id, name, creation, ranking, budget in billions, num employees\n",
      "- head: head id, name, born state, age\n",
      "- management: department id, head id, temporary acting\n",
      "\n",
      "FOREIGN_KEYS:\n",
      "- management.head id -> head.head id\n",
      "- management.department id -> department.department id\n",
      "\n",
      "ENTITIES:\n",
      "- VALUE '1990' -> None\n",
      "- TABLE 'departments' -> department\n",
      "- COLUMN 'created' -> department.creation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Test √âtapes 6 + 7 sur question_test\n",
    "# ===========================\n",
    "\n",
    "question_test = \"List all departments created before the year 1990.\"\n",
    "db_id_test = \"department_management\"\n",
    "\n",
    "# On fabrique un \"fake\" exemple au m√™me format que Spider\n",
    "example_test = {\n",
    "    \"question\": question_test,\n",
    "    \"db_id\": db_id_test,\n",
    "}\n",
    "\n",
    "# √âtape 6 : construire le contexte\n",
    "ctx_test = build_model_context(example_test, tables, nlp)\n",
    "\n",
    "# √âtape 7 : construire l'input texte pour BART\n",
    "model_input_test = build_model_input(\n",
    "    question=ctx_test.question_original,\n",
    "    db_id=ctx_test.db_id,\n",
    "    q_preproc=ctx_test.question_preproc,\n",
    "    norm_schema=ctx_test.schema_norm,\n",
    "    schema_entities=ctx_test.schema_entities,\n",
    "    schema_raw=ctx_test.schema_raw,\n",
    ")\n",
    "\n",
    "print(\"===== INPUT POUR BART (question_test) =====\")\n",
    "print(model_input_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "791cb1f9-3e4f-48bc-9387-5892d068fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "###√âTAPE 7 ‚Äî Pr√©paration du dataset pour entra√Æner BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6b0ede9a-08e0-4be0-b2f4-cbc01c52357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_pairs_from_file(json_path: str, tables, nlp):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    pairs = []\n",
    "    for example in data:\n",
    "        ctx = build_model_context(example, tables, nlp)\n",
    "        model_input = build_model_input(\n",
    "            question=ctx.question_original,\n",
    "            db_id=ctx.db_id,\n",
    "            q_preproc=ctx.question_preproc,\n",
    "            norm_schema=ctx.schema_norm,\n",
    "            schema_entities=ctx.schema_entities,\n",
    "            schema_raw=ctx.schema_raw,\n",
    "        )\n",
    "        gold_sql = example[\"query\"]\n",
    "        pairs.append({\n",
    "            \"input\": model_input,\n",
    "            \"output\": gold_sql,\n",
    "            \"db_id\": ctx.db_id\n",
    "        })\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ff97d663-dfe2-46de-b1f1-7318c1592947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 1659 8659 1034\n"
     ]
    }
   ],
   "source": [
    "# Paires pour train_spider\n",
    "pairs_train_spider = build_pairs_from_file(\"datafinal/train_spider.json\", tables, nlp)\n",
    "\n",
    "# Paires pour train_others\n",
    "pairs_train_others = build_pairs_from_file(\"datafinal/train_others.json\", tables, nlp)\n",
    "\n",
    "# Paires pour dev\n",
    "pairs_dev = build_pairs_from_file(\"datafinal/dev.json\", tables, nlp)\n",
    "\n",
    "# Train final = spider + others\n",
    "pairs_train_full = pairs_train_spider + pairs_train_others\n",
    "\n",
    "print(len(pairs_train_spider), len(pairs_train_others), len(pairs_train_full), len(pairs_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8c0b2d84-db90-4248-9dbe-99456950c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers cr√©√©s : bart_train_pairs.json et bart_dev_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Sauvegarder les paires d'entra√Ænement (train_spider + train_others)\n",
    "with open(\"bart_train_pairsfinal.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pairs_train_full, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Sauvegarder les paires de validation (dev.json)\n",
    "with open(\"bart_dev_pairsfinal.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pairs_dev, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Fichiers cr√©√©s : bart_train_pairs.json et bart_dev_pairs.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86157211-2fb7-41fd-821d-011311307183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
